---
title: "2020 Overwatch League Analysis"
output: html
runtime: shiny
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)
library(sf)
library(rnaturalearth)
library(shiny)
library(caret)
library(randomForest)
library(gbm)


knitr::opts_chunk$set(warning = FALSE)

tank = "#ff7eb6" # IBM Magenta 40
damage = "#4589ff" # IBM Blue 50
support = "#3ddbd9" # IBM Teal 30
```

## Introduction to Overwatch (2020)

Overwatch is a team team-based shooter game with many heros that each come with their own abilities. In 2020, Overwatch League's gameplay involved teams of six engaging in 6v6 combat across various maps and objectives.

The game categorizes characters into three roles:

- Tanks: These characters are the front line of every team. Tanks make space, absorb incoming damage, and set up plays for their teammates.
- Damage: These characters are the follow up to the front line. They are either launching rockets from up behind cover or are deep behind enemy lines taking out their support.
- Support: These characters usually take a back seat when it comes to eliminating the enemy team, by healing their team they ensure that their front-liners can take whatever comes at them, without getting taken out.

## Overwatch League: 2020 Season Analysis

The analysis focuses on the 2020 Overwatch League season, played on Overwatch's original format with six-player teams. This season is notable for its strategic and player dynamics, providing insight into player performances and tactics before Overwatch transitioned to a 5v5 format in its sequel, Overwatch 2.

The shift to 5v5 in Overwatch 2, particularly impacting the role and balance of Tank characters, offers an interesting contrast to the 2020 season data.

### Player Representation

The map below represents player nationality of which most players are from South Korea. We can see that most players are either from East Asian countries or Western Countries.

```{python player nationality map}
import pandas as pd
import geopandas as gpd
import plotly.express as px

# Load the provided data
file_path = 'overwatch_league_player_nationalities_updated.csv'  # Replace with your file path
players_data = pd.read_csv(file_path)

# Check if all values in 'Representation' are integers and convert accordingly
if (players_data['Representation'] % 1 == 0).all():
    players_data['Representation'] = players_data['Representation'].astype(int)
else:
    players_data['Representation'] = players_data['Representation'].astype(float)

# Load world data
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

# Merge the world data with the players data
world_data = world.merge(players_data, left_on='name', right_on='Country / Region', how='left')

# Create the Plotly choropleth map
fig = px.choropleth(world_data,
                    geojson=world_data.geometry,
                    locations=world_data.index,
                    color="Representation",
                    color_continuous_scale="Viridis",
                    labels={'Representation':'Number of Players'})

fig.update_geos(fitbounds="locations")
```

```{r analysis setup, include=FALSE}
# Function to calculate the mode
Mode <- function(x) {
    ux <- unique(x)
    if (length(ux) == 1) {
        return(ux)
    } # If there's only one unique value, return it
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
}

# Load the filtered dataset without "All Heroes"
file_path <- "phs_2020_1_no_all_heroes.csv"
data <- read_csv(file_path)

# Mapping of hero names to their roles
hero_roles <- c(
    "D.Va" = "Tank", "Doomfist" = "Tank", "Junker Queen" = "Tank", "Mauga" = "Tank",
    "Orisa" = "Tank", "Ramattra" = "Tank", "Reinhardt" = "Tank", "Roadhog" = "Tank",
    "Sigma" = "Tank", "Winston" = "Tank", "Wrecking Ball" = "Tank", "Zarya" = "Tank",
    "Ashe" = "Damage", "Bastion" = "Damage", "McCree" = "Damage", "Echo" = "Damage",
    "Genji" = "Damage", "Hanzo" = "Damage", "Junkrat" = "Damage", "Mei" = "Damage",
    "Pharah" = "Damage", "Reaper" = "Damage", "Sojourn" = "Damage",
    "Soldier: 76" = "Damage", "Sombra" = "Damage", "Symmetra" = "Damage",
    "Torbjörn" = "Damage", "Tracer" = "Damage", "Widowmaker" = "Damage",
    "Ana" = "Support", "Baptiste" = "Support", "Brigitte" = "Support",
    "Illari" = "Support", "Kiriko" = "Support", "Lifeweaver" = "Support",
    "Lúcio" = "Support", "Mercy" = "Support", "Moira" = "Support", "Zenyatta" = "Support"
)

# Apply the role mapping to the eliminations data
data <- data %>%
    mutate(role = hero_roles[hero_name])

# Filter the dataset for rows where 'stat_name' is 'Eliminations'
eliminations_data <- filter(data, stat_name == "Eliminations")

# Calculate the average eliminations per game for each player
average_eliminations <- eliminations_data %>%
    group_by(player_name, esports_match_id) %>%
    summarise(avg_elim = mean(stat_amount), .groups = "drop") %>%
    group_by(player_name) %>%
    summarise(average_eliminations = mean(avg_elim), .groups = "drop")

# Determine the most common role for each player
player_roles <- eliminations_data %>%
    group_by(player_name) %>%
    summarise(common_role = Mode(role), .groups = "drop")

# Merge the player roles with the average eliminations
player_impact <- merge(average_eliminations, player_roles, by = "player_name")
```

## Player Performance Analysis

My initial thought to analyze performance of players and heros is to examine their average eliminations per game. This will give us a good idea of who can best secure eliminations which are an important part of winnning not only team fights, but maps as a whole.

### Top Players by Eliminations

To identify the top players based on average eliminations per game, we have used a bar graph showing the top 20 players alongside the number of games played and sorted by color so we know if one role is better at achieving higher average eliminations than other roles. Including the number of games played can highlight outliers such as ChipSa who ranks number 3 in the league but only played one game all season.

```{r 1}
num_games_per_player <- eliminations_data %>%
    group_by(player_name) %>%
    summarise(num_games = n_distinct(esports_match_id), .groups = "drop")

player_impact <- merge(player_impact, num_games_per_player, by = "player_name")

top_20_players <- player_impact %>%
    arrange(desc(average_eliminations)) %>%
    slice_head(n = 20)

top_20_players$dummy_legend <- "Number of Games"

ggplot(top_20_players, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = common_role)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    scale_color_manual(values = "black", name = "", labels = "Number of Games") +
    coord_flip() +
    theme_minimal() +
    labs(x = "Player Name", y = "Average Eliminations Per Game", fill = "Player Role", title = "Top 20 Players by Average Eliminations Per Game") +
    theme(legend.position = "bottom", legend.title.align = 0.5)
```

### Top Players in each Role Compared

Again we use a bar graph to identify the top players based on elimination average but this time we are separating them into the top 5 based on role. This gives us a much clearer understanding compared with the previous graph that tanks generally have higher average eliminations while supports have the lowest.

```{r 5}
# Calculate the number of games for each player
num_games_per_player <- eliminations_data %>%
    group_by(player_name) %>%
    summarise(num_games = n_distinct(esports_match_id), .groups = "drop")

# Get the top 5 players within each role separately
top_players_by_role <- eliminations_data %>%
    group_by(role, player_name) %>%
    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = "drop") %>%
    group_by(role) %>% # Ensure the slicing is done within each role
    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %>%
    ungroup() %>%
    arrange(role, desc(average_eliminations))

# Merge this information with the number of games
top_players_by_role <- merge(top_players_by_role, num_games_per_player, by = "player_name")

# Create a dummy variable for the legend (for the number of games)
top_players_by_role$dummy_legend <- "Number of Games"

ggplot(top_players_by_role, aes(x = reorder(player_name, average_eliminations), y = average_eliminations, fill = role)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
    geom_text(aes(label = num_games, color = dummy_legend), hjust = -0.3, vjust = 0.5, position = position_dodge(width = 0.9)) +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    scale_color_manual(values = "black", name = "", labels = "Number of Games") +
    coord_flip() +
    labs(title = "Top 5 Players by Average Eliminations within Each Role", x = "Player", y = "Average Eliminations") +
    theme_minimal() +
    theme(legend.position = "bottom", legend.title.align = 0.5)

```

### Top Players in each Role Comparing all Roles played

This graph allows us to see which roles the players from the previous chart played best on in terms of average eliminations. This also gives us a better understanding of how these top players performed over a longer time. by showing their extremes as well as their first and third quartiles.

```{r 7}
# Identifying top 5 players in each role based on average eliminations
top_players_by_role <- eliminations_data %>%
    group_by(player_name, role) %>%
    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE), .groups = "drop") %>%
    group_by(role) %>%
    slice_max(order_by = average_eliminations, n = 5, with_ties = FALSE) %>%
    ungroup() %>%
    select(player_name)

# Subsetting the eliminations_data to include only these top players
top_eliminations_data <- eliminations_data %>%
    semi_join(top_players_by_role, by = "player_name")

# Creating the boxplot with the subsetted data
ggplot(top_eliminations_data, aes(x = player_name, y = stat_amount, fill = role)) +
    geom_boxplot(outlier.shape = NA) + # Optional: Hide outliers
    coord_flip() +
    labs(title = "Boxplot of Eliminations by Top 5 Players in Each Role", x = "Player", y = "Number of Eliminations") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    theme_minimal() +
    theme(legend.position = "bottom", axis.text.y = element_text(size = 7))

```

## Hero Performace Analysis

### Hero Elimination Impact

The objective of this graph is to understand which heros stand out when it comes to average eliminations. We can see that Damage and Tank heros generally trend towards the higher end of the graph while Supports who are mostly focused on healing and keeping their team alive have less focus spent on getting eliminations. Moira stands out as a support in the number 3 spot which can be explained by her kit which requires Moira to deal damage to regenerate her healing capibilities.

```{r 2a, output=FALSE}
avg_elim_by_hero <- eliminations_data %>%
    group_by(hero_name, role) %>% # Assuming there's a 'role' column
    summarise(average_eliminations = mean(stat_amount, na.rm = TRUE)) %>%
    ungroup() %>% # Remove grouping
    arrange(desc(average_eliminations))

ggplot(avg_elim_by_hero, aes(x = reorder(hero_name, average_eliminations), y = average_eliminations, fill = role)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    coord_flip() +
    labs(title = "Average Eliminations by Hero", x = "Hero", y = "Average Eliminations", fill = "Role") +
    theme_minimal() +
    theme(legend.position = "bottom")
```

```{r 2b, echo=FALSE}
ggplot(avg_elim_by_hero, aes(x = reorder(hero_name, average_eliminations), y = average_eliminations, fill = role)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    coord_flip() +
    labs(title = "Average Eliminations by Hero", x = "Hero", y = "Average Eliminations", fill = "Role") +
    theme_minimal() +
    theme(legend.position = "bottom")
```

### Elimination Distribution

This graph shows us the distribution of eliminations from game to game which highlights the differences between the Tank, Damage, and Support roles.


```{r 4}
ggplot(eliminations_data, aes(x = stat_amount, fill = role)) +
    geom_histogram(bins = 30, alpha = 0.7, position = "dodge") +
    labs(title = "Elimination Count Distribution by Role", x = "Number of Eliminations", y = "Frequency") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    theme_minimal()
```

### Hero Performance by Elimination Boxplot

These box plots allow us to visualize hero performance based on eliminations per game so we can see which hero's perform better, often. This also allows us to see which outliers are effecting the averages. We can also see some hero's that are not best evaluated by eliminations such as Mercy who is a primarily healing focused support hero.

```{r 6}
ggplot(eliminations_data, aes(x = hero_name, y = stat_amount, fill = role)) +
    geom_boxplot() +
    coord_flip() +
    labs(title = "Boxplot of Eliminations by Hero", x = "Hero", y = "Number of Eliminations") +
    theme_minimal()+
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support))
```

### Role Performance Boxplot

This box plot will allow us to visualize which role is having the most elimination impact over the entire league. We can see that there are some outlying games but overall the Damage role's top 50% is much larger than the others however Tank covers a little bit of a wider range.

We can also see that Tanks have a higher average than both Support and Damage.

```{r 8}
ggplot(eliminations_data, aes(x = role, y = stat_amount, fill = role)) +
    geom_boxplot() +
    labs(title = "Boxplot of Eliminations by Role", x = "Role", y = "Number of Eliminations") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    theme_minimal()
```

## General Analysises

### Map-Specific Hero Picks

Another very important aspect of the game is which map you are playing on. Teams who lose the previous map get to pick the next map to play on. This heat map allows us to visualize which heros are most played on which maps.

```{r 9}
hero_picks_count <- data %>%
    group_by(esports_match_id, map_name, hero_name) %>%
    summarise(count = n(), .groups = "drop") %>%
    ungroup()

hero_picks_aggregated <- hero_picks_count %>%
    group_by(map_name, hero_name) %>%
    summarise(total_count = sum(count), .groups = "drop")

hero_picks_wide <- hero_picks_aggregated %>%
    pivot_wider(names_from = map_name, values_from = total_count, values_fill = list(total_count = 0))

hero_picks_long <- hero_picks_wide %>%
    gather(key = "map_name", value = "pick_count", -hero_name)

ggplot(hero_picks_long, aes(x = map_name, y = hero_name, fill = pick_count)) +
    geom_tile(color = "white") +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "Hero Picks on Different Maps", x = "Map", y = "Hero") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

We can see that Lucio, baptiste, Reinhardt, Tracer, Zenyatta, and Sigma were played on most maps. We can also see that Zarya, Roadhog, Junkrat and Bastion were not picked that much. 

This heat map also allows so to see that Havana, Horizon Lunar Colony, and Numbani were not picked as maps to play on as much as the others.


## Machine Learning

Looking at all the previous information and seeing that support heros like lucio were played all the time, leads us to ask the question: "Why?" Lucio appeared so low on the hero analysis earlier but he was one of the most popular picks even though he didn't have much elimination impact.

To answer this question, we have trained multiple Machine Learning Models to determine which features are going to be the best to look at when determining Hero and Player performance. 

### Python
```{python ml, eval = FALSE}
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import numpy as np

# Load the dataset
data = pd.read_csv("merged_esports_data_updated.csv")

# Preprocess the data (fill missing values, one-hot encoding)
data = data.fillna(data.median(numeric_only=True))
data = pd.get_dummies(data, columns=["map_type", "map_name", "player_name", "team_name", "hero_name"])

# Drop irrelevant columns
data = data.drop(columns=["team_one_name", "team_two_name", "match_id"])

# Split the data into features and target
X = data.drop("match_winner", axis=1)
y = data["match_winner"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
lr_model = LogisticRegression().fit(X_train, y_train)
rf_model = RandomForestClassifier(n_estimators=50, random_state=42).fit(X_train, y_train)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)

# Predictions and Evaluation
predictions_lr = lr_model.predict(X_test)
predictions_rf = rf_model.predict(X_test)
predictions_gb = gb_model.predict(X_test)

# Save predictions and true values for R
results = pd.DataFrame({
    "y_test": y_test,
    "predictions_lr": predictions_lr,
    "predictions_rf": predictions_rf,
    "predictions_gb": predictions_gb
})
results.to_csv("model_results.csv", index=False)

# Save feature importances for R
feature_importances = pd.DataFrame({
    "feature": X.columns,
    "importance_rf": rf_model.feature_importances_,
    "importance_gb": gb_model.feature_importances_
})
feature_importances.to_csv("feature_importances.csv", index=False)

```

### R
```{r ml2}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)



# Read the model results and feature importances
results <- read.csv("model_results.csv")
feature_importances <- read.csv("feature_importances.csv")


# Selecting the top 10 important features for each model
top_features_rf <- feature_importances %>%
                   arrange(desc(importance_rf)) %>%
                   head(10)

top_features_gb <- feature_importances %>%
                   arrange(desc(importance_gb)) %>%
                   head(10)

# Confusion matrices and performance metrics
# Logistic Regression
cm_lr <- confusionMatrix(as.factor(results$predictions_lr), as.factor(results$y_test))
metrics_lr <- data.frame(Model = "Logistic Regression", Accuracy = cm_lr$overall['Accuracy'], Sensitivity = cm_lr$byClass['Sensitivity'], Specificity = cm_lr$byClass['Specificity'])

# Random Forest
cm_rf <- confusionMatrix(as.factor(results$predictions_rf), as.factor(results$y_test))
metrics_rf <- data.frame(Model = "Random Forest", Accuracy = cm_rf$overall['Accuracy'], Sensitivity = cm_rf$byClass['Sensitivity'], Specificity = cm_rf$byClass['Specificity'])

# Gradient Boosting
cm_gb <- confusionMatrix(as.factor(results$predictions_gb), as.factor(results$y_test))
metrics_gb <- data.frame(Model = "Gradient Boosting", Accuracy = cm_gb$overall['Accuracy'], Sensitivity = cm_gb$byClass['Sensitivity'], Specificity = cm_gb$byClass['Specificity'])

# Combine metrics and melt for ggplot
combined_metrics <- rbind(metrics_lr, metrics_rf, metrics_gb)
melted_metrics <- melt(combined_metrics, id.vars = "Model")

# Bar plot of accuracy, sensitivity, and specificity for each model
ggplot(melted_metrics, aes(x = variable, y = value, fill = variable)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    facet_wrap(~Model) +
    scale_fill_manual(values = c("Accuracy" = tank, "Sensitivity" = damage, "Specificity" = support)) +
    labs(x = "Metric", y = "Value", title = "Model Performance Comparison") +
    theme_minimal()

```

From analyzing the feature importance, accuracy, sensitivity, and specificity of each model, we can see that the Random Forest model does the best job at correctly predicting which team is going to win. 

```{r feature importance graph}
# Random Forest
ggplot(top_features_rf, aes(x = reorder(feature, importance_rf), y = importance_rf)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = "Top 10 Feature Importances - Random Forest")
```

The Random Forest Model uses the in game statistics to determine which team is going to win, while the Gradient Boosting model mostly just picks it's favorite team. Logistic Regression couldn't identify any true negatives, and gradient boosting did not do that great either. So for the highest accuracy, and the most realistic true positive and best true negative predicting, Random Forest stands miles ahead of the other models trained.


Since the Random Forest machine learning model decided that average_time_alive was the best feature to determine which team was going to win, I want to create a box plot showing which heros were able to survive longer on average.

```{r average time alive box plot}
time_alive_data <- filter(data, stat_name == "Average Time Alive")


ggplot(time_alive_data, aes(x = hero_name, y = stat_amount, fill = role)) +
    geom_boxplot() +
    coord_flip() +
    labs(title = "Boxplot of Average Time Alive by Hero", x = "Hero", y = "Average Time Alive") +
    theme_minimal()+
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support))
```

```{r}
avg_time_alive_by_hero <- time_alive_data %>%
    group_by(hero_name, role) %>% # Assuming there's a 'role' column
    summarise(average_time_alive = mean(stat_amount, na.rm = TRUE)) %>%
    ungroup() %>% # Remove grouping
    arrange(desc(average_time_alive))

ggplot(avg_time_alive_by_hero, aes(x = reorder(hero_name, average_time_alive), y = average_time_alive, fill = role)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    coord_flip() +
    labs(title = "Average Time Alive by Hero", x = "Hero", y = "Average Time Alive", fill = "Role") +
    theme_minimal() +
    theme(legend.position = "bottom")
```

These graphs explain some of the other picks such as Mercy and Sombra, and also reinforce D.Va as one of our top picks but lets look at the `objective-time` and `healing_done` features to get a better picture. 

```{r objective time graphs}
objective_time_data <- filter(data, stat_name == "Objective Time")
ggplot(objective_time_data, aes(x = hero_name, y = stat_amount, fill = role)) +
    geom_boxplot() +
    coord_flip() +
    labs(title = "Boxplot of Objective Time by Hero", x = "Hero", y = "Objective Time") +
    theme_minimal() +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support))

avg_objective_time_by_hero <- objective_time_data %>%
    group_by(hero_name, role) %>%
    summarise(average_objective_time = mean(stat_amount, na.rm = TRUE)) %>%
    ungroup() %>%
    arrange(desc(average_objective_time))

ggplot(avg_objective_time_by_hero, aes(x = reorder(hero_name, average_objective_time), y = average_objective_time, fill = role)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    coord_flip() +
    labs(title = "Average Objective Time by Hero", x = "Hero", y = "Average Objective Time", fill = "Role") +
    theme_minimal() +
    theme(legend.position = "bottom")
```

This gives us a better understanding of why Lucio is higher on the pick list. It also gives us some interesting information about certain Tanks that hold space on objectives really well, or on heros that can go solo contest an objective while the rest of the team goes to fight somewhere else with a positional advantage.

Lets look at `healing_done` to get a full picture.

```{r healing done plots}
healing_done_data <- filter(data, stat_name == "Healing Done")


ggplot(healing_done_data, aes(x = hero_name, y = stat_amount, fill = role)) +
    geom_boxplot() +
    coord_flip() +
    labs(title = "Boxplot of Healing Done by Hero", x = "Hero", y = "Healing Done") +
    theme_minimal() +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support))

avg_healing_done_by_hero <- healing_done_data %>%
    group_by(hero_name, role) %>%
    summarise(average_healing_done = mean(stat_amount, na.rm = TRUE)) %>%
    ungroup() %>%
    arrange(desc(average_healing_done))

ggplot(avg_healing_done_by_hero, aes(x = reorder(hero_name, average_healing_done), y = average_healing_done, fill = role)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values = c("Tank" = tank, "Damage" = damage, "Support" = support)) +
    coord_flip() +
    labs(title = "Average Healing Done by Hero", x = "Hero", y = "Average Healing Done", fill = "Role") +
    theme_minimal() +
    theme(legend.position = "bottom")
```
